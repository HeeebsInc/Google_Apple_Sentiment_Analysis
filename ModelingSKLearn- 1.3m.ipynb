{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm \n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import enchant\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def import_tweet_data():\n",
    "    \"\"\"Imports the tweet data from the 'data' folder, \n",
    "    with ISO-8859-1 encoding.\n",
    "    \n",
    "    Output: A Pandas DataFrame\"\"\"\n",
    "    \n",
    "    df = pd.read_csv('data/TweetsOriginal.csv', encoding = 'ISO-8859-1' )\n",
    "    return df\n",
    "\n",
    "def encode_emotion_3(x): \n",
    "    x = x.lower() \n",
    "    if x == 'negative emotion': \n",
    "        return 0 \n",
    "    elif x == 'no emotion toward brand or product': \n",
    "        return 2\n",
    "    elif x == 'positive emotion': \n",
    "        return 1\n",
    "    else: \n",
    "        return None\n",
    "    \n",
    "    \n",
    "def encode_emotion_2(x): \n",
    "    x = x.lower() \n",
    "    if x == 'negative emotion': \n",
    "        return 0 \n",
    "    elif x == 'positive emotion': \n",
    "        return 1\n",
    "    else: \n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "def clean_split(split_type, df): \n",
    "    new_df = pd.DataFrame() \n",
    "    new_df['Text'] = df['tweet_text']\n",
    "    new_df['Emotion'] = df['is_there_an_emotion_directed_at_a_brand_or_product']\n",
    "    if split_type == 2: \n",
    "        new_df['Emotion_New'] = new_df.Emotion.map(encode_emotion_2)\n",
    "    else: \n",
    "        new_df['Emotion_New'] = new_df.Emotion.map(encode_emotion_3)\n",
    "    \n",
    "    #dropping na in columns Text and Emotion\n",
    "    new_df.dropna(subset = ['Text', 'Emotion_New'], inplace = True)\n",
    "    \n",
    "    #getting rid of @ symbols\n",
    "    en_us = enchant.Dict(\"en_US\")\n",
    "\n",
    "    phrases = new_df.Text.values\n",
    "\n",
    "    for i, phrase in enumerate(new_df.Text):\n",
    "        phrases[i] = ' '.join(w for w in phrase.split() if en_us.check(w))\n",
    "\n",
    "    new_df.Text = phrases\n",
    "    \n",
    "    word_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tweet_token = TweetTokenizer()\n",
    "    new_df.Text = new_df.Text.map(lambda x: tweet_token.tokenize(x.lower()))\n",
    "    new_df.Text = new_df.Text.map(lambda x: ' '.join(x))\n",
    "    new_df.Text= new_df.Text.map(lambda x: word_tokenizer.tokenize(x.lower()))\n",
    "    new_df.Text = new_df.Text.map(lambda x: ' '.join(x))\n",
    "    \n",
    "    if split_type == 2:\n",
    "        print('Original Value Counts')\n",
    "        print(new_df.Emotion_New.value_counts())\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        pos_df = new_df[new_df.Emotion_New == 1]\n",
    "        neg_df = new_df[new_df.Emotion_New == 0]\n",
    "        \n",
    "        resample_pos = resample(pos_df, n_samples = 600, random_state = 10, replace = False)\n",
    "        new_df = resample_pos.append(neg_df, ignore_index = True)\n",
    "        print('Final Resampled Value Counts')\n",
    "        print(new_df.Emotion_New.value_counts())\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    \n",
    "    else: \n",
    "        print('Original Value Counts')\n",
    "        print(new_df.Emotion_New.value_counts())\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "           \n",
    "\n",
    "        pos_df = new_df[new_df.Emotion_New == 1]\n",
    "        neg_df = new_df[new_df.Emotion_New == 0]\n",
    "        neut_df = new_df[new_df.Emotion_New == 2]\n",
    "\n",
    "        resample_pos = resample(pos_df, n_samples = 600, random_state = 10, replace = False)\n",
    "        resample_neut = resample(neut_df, n_samples = 600, random_state = 10, replace = False)\n",
    "        \n",
    "        new_df = neg_df.append(resample_pos, ignore_index = True)\n",
    "        new_df = new_df.append(resample_neut, ignore_index = True)\n",
    "        print('Final Resampled Value Counts')\n",
    "        print(new_df.Emotion_New.value_counts())\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "    \n",
    "    #split into test and trains\n",
    "    x_train, x_test, y_train, y_test = train_test_split(new_df.Text, new_df.Emotion_New, stratify = new_df.Emotion_New,                                        \n",
    "                                                        train_size = .85, random_state = 10)\n",
    "    \n",
    "    #removing stop words\n",
    "    stop = stopwords.words('english')\n",
    "    vectorizer = CountVectorizer(stop_words = stop, max_features = 5000, ngram_range=(1,3))\n",
    "    clean_train = x_train.values\n",
    "    clean_test = x_test.values\n",
    "\n",
    "    train_features =vectorizer.fit_transform(clean_train).toarray()\n",
    "    test_features = vectorizer.fit_transform(clean_test).toarray()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #pickling\n",
    "    pickle.dump(train_features, open(f'../Pickles/{split_type}_x_train.p', 'wb'))\n",
    "    pickle.dump(test_features, open(f'../Pickles/{split_type}_x_test.p', 'wb'))\n",
    "    pickle.dump(y_train, open(f'../Pickles/{split_type}_y_train.p', 'wb'))\n",
    "    pickle.dump(y_test, open(f'../Pickles/{split_type}_y_test.p', 'wb'))\n",
    "    \n",
    "    print('Finished Pickling')\n",
    "    \n",
    "    \n",
    "    return train_features, test_features, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../1.3mTweets.csv', encoding = 'latin', header= None)\n",
    "df['emotion'] = df[0]\n",
    "df['tweet'] =  df[5]\n",
    "df = df[['emotion', 'tweet']]\n",
    "\n",
    "\n",
    "df.emotion = df.emotion.replace(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    800000\n",
       "0    800000\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1600000/1600000 [59:52<00:00, 445.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "en_us = enchant.Dict(\"en_US\")\n",
    "\n",
    "phrases = df.tweet.values\n",
    "pbar = tqdm(enumerate(df.tweet), total = len(df.tweet))\n",
    "for i, phrase in pbar:\n",
    "    phrases[i] = ' '.join(w for w in phrase.split() if en_us.check(w))\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tweet = phrases\n",
    "\n",
    "word_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tweet_token = TweetTokenizer()\n",
    "df.tweet = df.tweet.map(lambda x: tweet_token.tokenize(x.lower()))\n",
    "df.tweet = df.tweet.map(lambda x: ' '.join(x))\n",
    "df.tweet= df.tweet.map(lambda x: word_tokenizer.tokenize(x.lower()))\n",
    "df.tweet = df.tweet.map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../Cleaned1.3m.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(new_df.Text, new_df.Emotion_New, stratify = new_df.Emotion_New,                                        \n",
    "                                                        train_size = .85, random_state = 10)\n",
    "\n",
    "#removing stop words\n",
    "stop = stopwords.words('english')\n",
    "vectorizer = CountVectorizer(stop_words = stop, max_features = 5000, ngram_range=(1,3))\n",
    "clean_train = x_train.values\n",
    "clean_test = x_test.values\n",
    "\n",
    "train_features =vectorizer.fit_transform(clean_train).toarray()\n",
    "test_features = vectorizer.fit_transform(clean_test).toarray()\n",
    "\n",
    "\n",
    "\n",
    "#pickling\n",
    "pickle.dump(train_features, open(f'../Pickles/test_x_train.p', 'wb'))\n",
    "pickle.dump(test_features, open(f'../Pickles/test_x_test.p', 'wb'))\n",
    "pickle.dump(y_train, open(f'../Pickles/test_y_train.p', 'wb'))\n",
    "pickle.dump(y_test, open(f'../Pickles/test_y_test.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, RidgeClassifier, LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "pd.options.display.max_rows = 35 \n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickles(split_type): \n",
    "    x_train = pickle.load(open(f'../Pickles/{split_type}_x_train.p', 'rb'))\n",
    "    x_test = pickle.load(open(f'../Pickles/{split_type}_x_test.p', 'rb'))\n",
    "    y_train = pickle.load(open(f'../Pickles/{split_type}_y_train.p', 'rb'))\n",
    "    y_test = pickle.load(open(f'../Pickles/{split_type}_y_test.p', 'rb'))\n",
    "    \n",
    "    \n",
    "    print('Train Value Counts')\n",
    "    print(y_train.value_counts())\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print('Test Value Counts')\n",
    "    print(y_test.value_counts())\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = get_pickles(test)\n",
    "print(x_train.shape, x_test.shape) \n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling\n",
    "- 0 = negative\n",
    "- 1 = Positive \n",
    "- 2 = neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'Log': LogisticRegression(), 'Knn': KNeighborsClassifier(), \n",
    "          'DT': DecisionTreeClassifier(random_state = 10), 'Gaussian': GaussianNB(), 'LDA': LinearDiscriminantAnalysis(),\n",
    "          'LinearSVC': LinearSVC(max_iter = 1250, random_state = 10), 'SDGSVC': SGDClassifier(random_state = 10),  \n",
    "          'ADA': AdaBoostClassifier(random_state = 10), 'Bagging': BaggingClassifier(random_state = 10), \n",
    "          'Ridge': RidgeClassifier(random_state = 10), \n",
    "          'RF': RandomForestClassifier(random_state = 10)}\n",
    "\n",
    "#create stacked model\n",
    "stack_m = [] \n",
    "for model, m in models.items(): \n",
    "    stack_m.append((model, m))\n",
    "stack_model = StackingClassifier(estimators = stack_m, final_estimator = LogisticRegression(), cv = 5)\n",
    "models['stacked'] = stack_model\n",
    "\n",
    "#test each model and stacking\n",
    "results = []\n",
    "model_names = []\n",
    "pbar = tqdm(models.items())\n",
    "for model, m in pbar: \n",
    "    pbar.set_description(f'Evaluating {model.upper()}')\n",
    "    cv = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 5)\n",
    "    scores = cross_val_score(m, x_train, y_train, scoring = 'accuracy', cv = cv, n_jobs = 13, \n",
    "                             error_score = 'raise')\n",
    "    results.append(scores)\n",
    "    model_names.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_dict = {i:y for i,y in zip(model_names, results)}\n",
    "pickle.dump(vanilla_dict, open('models/VanillaResults.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "plt.boxplot(results, labels = model_names, showmeans = True)\n",
    "plt.title('Accuracy for Each Vanilla Model (Version 1)')\n",
    "plt.ylabel('Accuracy'); plt.xlabel('Model')\n",
    "plt.savefig('figures/BaselineAccuracy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'Log': LogisticRegression(), \n",
    "          'Gaussian': GaussianNB(), \n",
    "          'LinearSVC': LinearSVC(max_iter = 1250, random_state = 10), 'SDGSVC': SGDClassifier(random_state = 10),  \n",
    "          'Ridge': RidgeClassifier(random_state = 10), 'RF': RandomForestClassifier(random_state = 10)}\n",
    "\n",
    "#create stacked model\n",
    "stack_m = [] \n",
    "for model, m in models.items(): \n",
    "    stack_m.append((model, m))\n",
    "stack_model = StackingClassifier(estimators = stack_m, final_estimator = LogisticRegression(), cv = 5)\n",
    "models['stacked'] = stack_model\n",
    "\n",
    "#test each model and stacking\n",
    "results = []\n",
    "model_names = []\n",
    "pbar = tqdm(models.items())\n",
    "for model, m in pbar: \n",
    "    pbar.set_description(f'Evaluating {model.upper()}')\n",
    "    cv = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 5)\n",
    "    scores = cross_val_score(m, x_train, y_train, scoring = 'accuracy', cv = cv, n_jobs = -1, \n",
    "                             error_score = 'raise')\n",
    "    results.append(scores)\n",
    "    model_names.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_dict = {i:y for i,y in zip(model_names, results)}\n",
    "pickle.dump(vanilla_dict, open('models/VanillaResults2.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "plt.boxplot(results, labels = model_names, showmeans = True)\n",
    "plt.title('Accuracy for Each Vanilla Model (Version 2)')\n",
    "plt.ylabel('Accuracy'); plt.xlabel('Model')\n",
    "plt.savefig('figures/BaselineAccuracy2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'Log': LogisticRegression(), \n",
    "          'Gaussian': GaussianNB(), \n",
    "          'LinearSVC': LinearSVC(max_iter = 1250, random_state = 10), 'Ridge': RidgeClassifier(random_state = 10), \n",
    "          'RF': RandomForestClassifier(random_state = 10)}\n",
    "\n",
    "#create stacked model\n",
    "stack_m = [] \n",
    "for model, m in models.items(): \n",
    "    stack_m.append((model, m))\n",
    "stack_model = StackingClassifier(estimators = stack_m, final_estimator = LogisticRegression(), cv = 5)\n",
    "models['stacked'] = stack_model\n",
    "\n",
    "#test each model and stacking\n",
    "results = []\n",
    "model_names = []\n",
    "pbar = tqdm(models.items())\n",
    "for model, m in pbar: \n",
    "    pbar.set_description(f'Evaluating {model.upper()}')\n",
    "    cv = RepeatedStratifiedKFold(n_splits = 7, n_repeats = 10)\n",
    "    scores = cross_val_score(m, x_train, y_train, scoring = 'accuracy', cv = cv, n_jobs = -1, \n",
    "                             error_score = 'raise')\n",
    "    results.append(scores)\n",
    "    model_names.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_dict = {i:y for i,y in zip(model_names, results)}\n",
    "pickle.dump(vanilla_dict, open('models/VanillaResults3.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "plt.boxplot(results, labels = model_names, showmeans = True)\n",
    "plt.title('Accuracy for Each Vanilla Model (Version 3)')\n",
    "plt.ylabel('Accuracy'); plt.xlabel('Model')\n",
    "plt.savefig('figures/BaselineAccuracy2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
