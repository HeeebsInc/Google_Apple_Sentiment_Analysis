{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Functions import clean_split\n",
    "# pd.options.display.max_rows = 35 \n",
    "# pd.options.display.max_columns = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/TweetsOriginal.csv', encoding = 'ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Value Counts\n",
      "1.0    2978\n",
      "0.0     570\n",
      "Name: Emotion_New, dtype: int64\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Final Resampled Value Counts\n",
      "1.0    600\n",
      "0.0    570\n",
      "Name: Emotion_New, dtype: int64\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = clean_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Train: 0.9859154929577465\n",
      "Vanilla Test: 0.8181818181818182\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Vanilla Train: 0.9959758551307847\n",
      "Vanilla Test: 0.8125\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression \n",
    "\n",
    "log = LogisticRegression()\n",
    "log.fit(x_train, y_train) \n",
    "\n",
    "print(f'Vanilla Train: {log.score(x_train, y_train)}')\n",
    "print(f'Vanilla Test: {log.score(x_test, y_test)}')\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "\n",
    "\n",
    "ridge = RidgeClassifier(random_state = 10)\n",
    "ridge.fit(x_train, y_train) \n",
    "\n",
    "print(f'Vanilla Train: {ridge.score(x_train, y_train)}')\n",
    "print(f'Vanilla Test: {ridge.score(x_test, y_test)}')\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm \n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords, words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import enchant\n",
    "import pickle\n",
    "from sklearn.utils import resample\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, f1_score, auc\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def get_pickles(split_type): \n",
    "    x_train = pickle.load(open(f'../Pickles/{split_type}_x_train.p', 'rb'))\n",
    "    x_test = pickle.load(open(f'../Pickles/{split_type}_x_test.p', 'rb'))\n",
    "    y_train = pickle.load(open(f'../Pickles/{split_type}_y_train.p', 'rb'))\n",
    "    y_test = pickle.load(open(f'../Pickles/{split_type}_y_test.p', 'rb'))\n",
    "    \n",
    "    \n",
    "    print('Train Value Counts')\n",
    "    print(y_train.value_counts())\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print('Test Value Counts')\n",
    "    print(y_test.value_counts())\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def import_tweet_data():\n",
    "    \"\"\"Imports the tweet data from the 'data' folder, \n",
    "    with ISO-8859-1 encoding.\n",
    "    \n",
    "    Output: A Pandas DataFrame\"\"\"\n",
    "    \n",
    "    df = pd.read_csv('data/TweetsOriginal.csv', encoding = 'ISO-8859-1' )\n",
    "    return df\n",
    "\n",
    "def encode_emotion_3(x): \n",
    "    x = x.lower() \n",
    "    if x == 'negative emotion': \n",
    "        return 0 \n",
    "    elif x == 'no emotion toward brand or product': \n",
    "        return 2\n",
    "    elif x == 'positive emotion': \n",
    "        return 1\n",
    "    else: \n",
    "        return None\n",
    "    \n",
    "    \n",
    "def encode_emotion_2(x): \n",
    "    x = x.lower() \n",
    "    if x == 'negative emotion': \n",
    "        return 0 \n",
    "    elif x == 'positive emotion': \n",
    "        return 1\n",
    "    else: \n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "def clean_split(split_type, df): \n",
    "    new_df = pd.DataFrame() \n",
    "    new_df['Text'] = df['tweet_text']\n",
    "    new_df['Item'] = df['emotion_in_tweet_is_directed_at']\n",
    "    new_df['Emotion'] = df['is_there_an_emotion_directed_at_a_brand_or_product']\n",
    "    if split_type == 2: \n",
    "        new_df['Emotion_New'] = new_df.Emotion.map(encode_emotion_2)\n",
    "    else: \n",
    "        new_df['Emotion_New'] = new_df.Emotion.map(encode_emotion_3)\n",
    "    \n",
    "    #dropping na in columns Text and Emotion\n",
    "    new_df.dropna(subset = ['Text', 'Emotion_New'], inplace = True)\n",
    " \n",
    "    tweet_token = TweetTokenizer()\n",
    "\n",
    "    eng_words = set(words.words())\n",
    " \n",
    "    tweets = new_df.Text.values\n",
    "    new_tweets = []\n",
    "    for sent in tweets:\n",
    "        new_tweets.append(' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",sent.lower()).split()))\n",
    "    new_df.Text = new_tweets\n",
    "    \n",
    "    word_tokenizer = RegexpTokenizer(\"([a-zA-Z&]+(?:'[a-z]+)?)\")\n",
    "    word_lemmet = WordNetLemmatizer()\n",
    "    word_stem = PorterStemmer()\n",
    "    tweet_token = TweetTokenizer()\n",
    "#     new_df.Text = new_df.Text.map(lambda x: tweet_token.tokenize(x.lower()))\n",
    "#     new_df.Text = new_df.Text.map(lambda x: ' '.join(x))\n",
    "    new_df.Text= new_df.Text.map(lambda x: word_tokenizer.tokenize(x.lower()))\n",
    "    new_df.Text = new_df.Text.map(lambda x: ' '.join([word_stem.stem(i) for i in x if len(i) > 2]))\n",
    "#     new_df.Text = new_df.Text.map(lambda x: ' '.join([word_lemmet.lemmatize(i) for i in x if len(i) > 2]))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if split_type == 2:\n",
    "        print('Original Value Counts')\n",
    "        print(new_df.Emotion_New.value_counts())\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        pos_df = new_df[new_df.Emotion_New == 1]\n",
    "        neg_df = new_df[new_df.Emotion_New == 0]\n",
    "        \n",
    "        resample_pos = resample(pos_df, n_samples = 600, random_state = 10, replace = False)\n",
    "        new_df = resample_pos.append(neg_df, ignore_index = True)\n",
    "        print('Final Resampled Value Counts')\n",
    "        print(new_df.Emotion_New.value_counts())\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    \n",
    "    else: \n",
    "        print('Original Value Counts')\n",
    "        print(new_df.Emotion_New.value_counts())\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "           \n",
    "\n",
    "        pos_df = new_df[new_df.Emotion_New == 1]\n",
    "        neg_df = new_df[new_df.Emotion_New == 0]\n",
    "        neut_df = new_df[new_df.Emotion_New == 2]\n",
    "\n",
    "        resample_pos = resample(pos_df, n_samples = 600, random_state = 10, replace = False)\n",
    "        resample_neut = resample(neut_df, n_samples = 600, random_state = 10, replace = False)\n",
    "        \n",
    "        new_df = neg_df.append(resample_pos, ignore_index = True)\n",
    "        new_df = new_df.append(resample_neut, ignore_index = True)\n",
    "        print('Final Resampled Value Counts')\n",
    "        print(new_df.Emotion_New.value_counts())\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    #split into test and trains\n",
    "    x_train, x_test, y_train, y_test = train_test_split(new_df.Text, new_df.Emotion_New, stratify = new_df.Emotion_New,                                        \n",
    "                                                        train_size = .85, random_state = 10)\n",
    "    \n",
    "    #removing stop words\n",
    "    new_stop = ['abacus', 'yr', 'acerbic', 'bcet', 'beechwood', 'bicycle', 'brian', 'sxsw', 'ce', 'louis', 'mngr', \n",
    "               'rewardswagon', 'loui', 'csuitecsourc', 'wjchat', 'peter', 'bbq', 'au', 'austin', 'awesometim', 'bankinnov', \n",
    "                'barton', 'boooo', 'bookbook']\n",
    "    stop = stopwords.words('english') + new_stop\n",
    "    vectorizer= CountVectorizer(stop_words = stop, max_features = 6000, ngram_range=(1,2))\n",
    "#     vectorizer= TfidfVectorizer(stop_words = stop, max_features = 5000, ngram_range=(1,2))\n",
    "   \n",
    "    \n",
    "    clean_train = x_train.values\n",
    "    clean_test = x_test.values\n",
    "    vectorizer.fit(clean_train)\n",
    "    pickle.dump(vectorizer, open('../Pickles/vectorizer.p', 'wb'))\n",
    "\n",
    "    train_features =vectorizer.transform(clean_train).toarray()\n",
    "    test_features = vectorizer.transform(clean_test).toarray()\n",
    "    \n",
    "    train_df = pd.DataFrame(train_features, columns = vectorizer.get_feature_names())\n",
    "    train_df['target'] = y_train.values\n",
    "    \n",
    "    train_df.to_csv('data/TrainDF.csv', index = False)\n",
    "    test_df = pd.DataFrame(test_features, columns = vectorizer.get_feature_names())\n",
    "    test_df['target'] = y_test.values\n",
    "    \n",
    "    test_df.to_csv('data/TestDF.csv', index = False)\n",
    "    #pickling\n",
    "    pickle.dump(train_features, open(f'../Pickles/{split_type}_x_train.p', 'wb'))\n",
    "    pickle.dump(test_features, open(f'../Pickles/{split_type}_x_test.p', 'wb'))\n",
    "    pickle.dump(y_train, open(f'../Pickles/{split_type}_y_train.p', 'wb'))\n",
    "    pickle.dump(y_test, open(f'../Pickles/{split_type}_y_test.p', 'wb'))\n",
    "    \n",
    "    print('Finished Pickling')\n",
    "    \n",
    "    return train_features, test_features, y_train, y_test, train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = clean_split(2, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['abacus', 'yr', 'acerbic', 'bcet', 'beechwood', 'bicycle', 'brian', 'sxsw', 'ce', 'louis', 'mngr', \n",
    "               'rewardswagon', 'loui', 'csuitecsourc', 'wjchat', 'peter', 'bbq', 'au', 'austin', 'awesometim', 'bankinnov', \n",
    "                'barton', 'boooo', 'bookbook', 'zappo', 'zazzlesxsw', 'gswsxsw', 'tmsxsw', 'ye', \n",
    "                'xipad', 'xooom', 'agileag', 'whrrl', 'whi', 'marissamey', 'evolvingworkplac', 'fastsocieti', 'foursquar',\n",
    "                'justin', 'frankeninterfac', 'gowalla']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "\n",
    "x_train, x_test, y_train, y_test, test_df = clean_split(2, df)\n",
    "\n",
    "log = LogisticRegression()\n",
    "log.fit(x_train, y_train) \n",
    "\n",
    "print(f'Vanilla Train: {log.score(x_train, y_train)}')\n",
    "print(f'Vanilla Test: {log.score(x_test, y_test)}')\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "\n",
    "\n",
    "ridge = RidgeClassifier(random_state = 10)\n",
    "ridge.fit(x_train, y_train) \n",
    "\n",
    "print(f'Vanilla Train: {ridge.score(x_train, y_train)}')\n",
    "print(f'Vanilla Test: {ridge.score(x_test, y_test)}')\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 10, n_jobs = -1)\n",
    "rf.fit(x_train, y_train) \n",
    "\n",
    "print(f'Vanilla Train: {rf.score(x_train, y_train)}')\n",
    "print(f'Vanilla Test: {rf.score(x_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = test_df.columns.tolist()\n",
    "sum_values = test_df.sum() \n",
    "\n",
    "for c, s in zip(columns, sum_values): \n",
    "    print(c, s)\n",
    "    # for c, s in enum_sum: \n",
    "#     print(c, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'dst', 'amp', 'farm', 'fmsignal', 'sxswi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pickle = pickle.load(open('../Pickles/Forest_6_clf.p', 'rb'))\n",
    "rf_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pickle.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(criterion='entropy', max_depth=25, min_samples_leaf=2,\n",
    "                       oob_score=True, random_state=10)\n",
    "\n",
    "rf.fit(x_train, y_train)\n",
    "print(rf.score(x_train, y_train), rf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
